{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd522b0",
   "metadata": {
    "id": "50b9d2bc"
   },
   "source": [
    "## Chapter 3: Low-Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3396a",
   "metadata": {},
   "source": [
    "### Spoilers\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Understand what a low-rank adapter is and why itâ€™s useful\n",
    "- Prepare the quantized model for training\n",
    "- Use `peft` to create and attach adapters to a base model\n",
    "- Discuss configuration options for targeting layers for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b25f1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2f40c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kODUm5BmEQhI",
    "outputId": "5a17ca8b-855a-4e55-b7a2-ab2af98d8906"
   },
   "outputs": [],
   "source": [
    "# If you're running on Colab\n",
    "!pip install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running on runpod.io's Jupyter Template\n",
    "#!pip install transformers peft huggingface-hub accelerate safetensors pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2630dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556455b2",
   "metadata": {
    "id": "d6f0f0a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from numpy.linalg import matrix_rank\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836cf4f",
   "metadata": {},
   "source": [
    "### The Goal\n",
    "\n",
    "We attach adapters to the huge linear layers in an LLM to drastically reduce the number of trainable parameters. We can easily shrink the number of trainable parameters down to less than 1% of their original number. By reducing both computation (fewer gradients to compute) and memory footprint (fewer parameters tracked by the optimizer), we achieve significant efficiency gains. Keep in mind, however, that low-rank adapters are unlikely to match the performance of full-model tuning, and their effectiveness may vary depending on the base model and the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecd6d8",
   "metadata": {},
   "source": [
    "### Pre-Reqs\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/master/images/ch3/matmul.png)\n",
    "<center>Figure 3.1 - Matrix multiplication</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461eaab6",
   "metadata": {
    "id": "7a8047b8"
   },
   "source": [
    "### Low-Rank Adaptation in a Nutshell\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/master/images/ch3/two_matrices.png)\n",
    "<center>Figure 3.2 - Multiplying two low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05c5b83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea55fe5c",
    "outputId": "a571e499-7efe-4f28-c450-eae1dc5090bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_layer = nn.Linear(1024, 1024, bias=False)\n",
    "base_layer.weight.shape, base_layer.weight.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55651493",
   "metadata": {},
   "source": [
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/master/images/ch3/lowrank_matrices.png)\n",
    "<center>Figure 3.3 - Frozen weights and low-rank matrices</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0305a2fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efc2b9ea",
    "outputId": "eb191632-1c89-4d9a-e13a-f0fbf6c1d7b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=1024, out_features=8, bias=False),\n",
       " Linear(in_features=8, out_features=1024, bias=False))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(11)\n",
    "r = 8\n",
    "layer_A = nn.Linear(base_layer.in_features, r, bias=False)\n",
    "layer_B = nn.Linear(r, base_layer.out_features, bias=False)\n",
    "layer_A, layer_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e716c5f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f508efea",
    "outputId": "4b64e81f-97f2-48ea-d5f0-daa321262cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192, 8192)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_A.weight.numel(), layer_B.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2e401c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "877024f8",
    "outputId": "8d5ad510-c1e1-4ce3-e1da-cdb5d7cb54bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1024]), 1048576)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite = layer_B.weight @ layer_A.weight\n",
    "composite.shape, composite.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25358bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fd27724",
    "outputId": "2dc7217f-4f8e-49f2-c484-cac641021140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_rank(composite.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952840a7",
   "metadata": {
    "id": "71da61a0"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ (W + B @ A)^T\n",
    "$$\n",
    "<center>Equation 3.1 - Adding the resulting product to the weights</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b772a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ea66ae2",
    "outputId": "2107be40-6940-4bdc-e06d-003fa4d63e8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3740,  0.0485, -1.2761,  ...,  0.5605,  0.1853,  0.5227]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(19)\n",
    "batch = torch.randn(1, 1024)\n",
    "\n",
    "batch @ (base_layer.weight.data + layer_B.weight @ layer_A.weight).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4575839d",
   "metadata": {
    "id": "977dda5e"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = \\underbrace{X @ W^T}_{O_W} + \\underbrace{X @ (B @ A)^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.2 - Using two forward passes</center>\n",
    "\n",
    "![](https://github.com/dvgodoy/FineTuningLLMs/blob/master/images/ch3/forward.png)\n",
    "<center>Figure 3.4 - Using two forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68eefe07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8e9b9c3",
    "outputId": "900dca7e-4313-4056-9ebc-19fc846f86c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3383,  0.0255, -0.8154,  ...,  0.8525, -0.0091,  0.0186]]),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = batch @ base_layer.weight.data.T\n",
    "additional_output = batch @ (layer_B.weight @ layer_A.weight).T\n",
    "regular_output, additional_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f73e0",
   "metadata": {
    "id": "e7b0e320"
   },
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{additional} = X @ (B @ A)^T = \\underbrace{\\underbrace{(X @ A^T)}_{O_A} @ B^T}_{O_{AB}}\n",
    "$$\n",
    "<center>Equation 3.3 - Chaining the adapterâ€™s forward passes</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf28799e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4e9cd6e",
    "outputId": "1f6f2148-45cc-4c56-a9cb-35447d5c3c9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_A = (batch @ layer_A.weight.T)\n",
    "additional_output = out_A @ layer_B.weight.T\n",
    "additional_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82895a2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "267a2e77",
    "outputId": "8568a39b-f9a8-459e-cc1e-bcde1809d7db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3383,  0.0255, -0.8154,  ...,  0.8525, -0.0091,  0.0186]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.0357,  0.0230, -0.4607,  ..., -0.2920,  0.1944,  0.5041]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([[-0.3740,  0.0485, -1.2761,  ...,  0.5605,  0.1853,  0.5227]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_output = base_layer(batch)\n",
    "out_A = layer_A(batch)\n",
    "additional_output = layer_B(out_A)\n",
    "output = regular_output + additional_output\n",
    "regular_output, additional_output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea74713",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Large\n",
    "\\text{output} = X @ W^T + \\frac{\\alpha}{r}\\left[X @ (B @ A)^T\\right]\n",
    "$$\n",
    "<center>Equation 3.4 - LoRAâ€™s alpha</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dcb6e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4097,  0.0715, -1.7368,  ...,  0.2684,  0.3796,  1.0268]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 2*r\n",
    "output = regular_output + (alpha / r) * additional_output\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55483043",
   "metadata": {
    "id": "573594fd"
   },
   "source": [
    "### The Road So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b713bac",
   "metadata": {
    "id": "8fd8cca5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvgodoy/anaconda3/envs/pyt20/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "supported = torch.cuda.is_bf16_supported()#including_emulation=False)\n",
    "compute_dtype = (torch.bfloat16 if supported else torch.float32)\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "                                                device_map='auto',\n",
    "                                                torch_dtype=compute_dtype,\n",
    "                                                quantization_config=nf4_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9072b",
   "metadata": {
    "id": "a92ebdb7"
   },
   "source": [
    "### Parameter Types and Gradients\n",
    "\n",
    "****\n",
    "**Summary of \"Parameter Types and Gradients\"**\n",
    "- quantization only freezes the linear layers that have been quantized\n",
    "- after quantization, a model can be prepared using the `prepare_model_for_kbit_training()` function\n",
    "  - it freezes **all** layers\n",
    "  - it casts every non-quantized 16-bit layer to FP32 to improve training\n",
    "  - it enables gradient checkpointing\n",
    "- you'll be able to unfreeze layers of your choice later on using the LoRA configuration\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49a9abe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ade01343",
    "outputId": "aed6d7b3-59fa-475d-ab04-e0b890667d42",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decoder.embed_tokens.weight', torch.float32),\n",
       " ('decoder.embed_positions.weight', torch.float32),\n",
       " ('decoder.layers.0.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.0.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.0.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.0.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.1.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.1.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.1.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.1.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.2.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.2.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.2.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.2.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.3.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.3.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.3.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.3.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.4.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.4.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.4.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.4.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.5.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.5.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.5.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.5.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.6.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.6.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.6.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.6.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.7.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.7.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.7.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.7.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.8.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.8.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.8.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.8.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.9.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.9.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.9.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.9.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.10.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.10.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.10.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.10.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.11.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.11.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.11.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.11.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.12.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.12.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.12.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.12.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.13.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.13.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.13.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.13.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.14.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.14.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.14.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.14.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.15.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.15.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.15.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.15.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.16.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.16.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.16.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.16.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.17.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.17.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.17.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.17.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.18.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.18.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.18.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.18.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.19.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.19.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.19.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.19.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.20.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.20.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.20.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.20.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.21.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.21.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.21.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.21.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.22.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.22.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.22.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.22.final_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.23.self_attn_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.23.self_attn_layer_norm.bias', torch.float32),\n",
       " ('decoder.layers.23.final_layer_norm.weight', torch.float32),\n",
       " ('decoder.layers.23.final_layer_norm.bias', torch.float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trainable_parms(model):\n",
    "    parms = [(name, param.dtype) for name, param in model.named_parameters() if param.requires_grad]\n",
    "    return parms\n",
    "\n",
    "trainable_parms(model_q4.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2132acb9",
   "metadata": {
    "id": "93e47552"
   },
   "source": [
    "#### `prepare_model_for_kbit_training()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3455ebde",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7134e644",
    "outputId": "50e366cf-6587-4624-a6aa-256da7ecc249"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTForCausalLM(\n",
       "  (model): OPTModel(\n",
       "    (decoder): OPTDecoder(\n",
       "      (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "      (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "      (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x OPTDecoderLayer(\n",
       "          (self_attn): OPTAttention(\n",
       "            (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "                                        use_gradient_checkpointing=True,\n",
    "                                        gradient_checkpointing_kwargs={'use_reentrant': False})\n",
    "prepared_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dbad9bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ace6c02",
    "outputId": "e077a599-0833-4016-dffd-89831d5cf0f1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e8be25c",
   "metadata": {
    "id": "2f84b493"
   },
   "outputs": [],
   "source": [
    "def parms_of_dtype(model, dtype=torch.float32):\n",
    "    parms = [name for name, param in model.named_parameters() if param.dtype == dtype]\n",
    "    return parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653fa73d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de31ab68",
    "outputId": "f67523da-2737-4956-e8da-766ee8792d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.decoder.embed_tokens.weight',\n",
       " 'model.decoder.embed_positions.weight',\n",
       " 'model.decoder.layers.0.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.0.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.0.fc1.bias',\n",
       " 'model.decoder.layers.0.fc2.bias',\n",
       " 'model.decoder.layers.0.final_layer_norm.weight',\n",
       " 'model.decoder.layers.0.final_layer_norm.bias',\n",
       " 'model.decoder.layers.1.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.1.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.1.fc1.bias',\n",
       " 'model.decoder.layers.1.fc2.bias',\n",
       " 'model.decoder.layers.1.final_layer_norm.weight',\n",
       " 'model.decoder.layers.1.final_layer_norm.bias',\n",
       " 'model.decoder.layers.2.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.2.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.2.fc1.bias',\n",
       " 'model.decoder.layers.2.fc2.bias',\n",
       " 'model.decoder.layers.2.final_layer_norm.weight',\n",
       " 'model.decoder.layers.2.final_layer_norm.bias',\n",
       " 'model.decoder.layers.3.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.3.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.3.fc1.bias',\n",
       " 'model.decoder.layers.3.fc2.bias',\n",
       " 'model.decoder.layers.3.final_layer_norm.weight',\n",
       " 'model.decoder.layers.3.final_layer_norm.bias',\n",
       " 'model.decoder.layers.4.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.4.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.4.fc1.bias',\n",
       " 'model.decoder.layers.4.fc2.bias',\n",
       " 'model.decoder.layers.4.final_layer_norm.weight',\n",
       " 'model.decoder.layers.4.final_layer_norm.bias',\n",
       " 'model.decoder.layers.5.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.5.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.5.fc1.bias',\n",
       " 'model.decoder.layers.5.fc2.bias',\n",
       " 'model.decoder.layers.5.final_layer_norm.weight',\n",
       " 'model.decoder.layers.5.final_layer_norm.bias',\n",
       " 'model.decoder.layers.6.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.6.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.6.fc1.bias',\n",
       " 'model.decoder.layers.6.fc2.bias',\n",
       " 'model.decoder.layers.6.final_layer_norm.weight',\n",
       " 'model.decoder.layers.6.final_layer_norm.bias',\n",
       " 'model.decoder.layers.7.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.7.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.7.fc1.bias',\n",
       " 'model.decoder.layers.7.fc2.bias',\n",
       " 'model.decoder.layers.7.final_layer_norm.weight',\n",
       " 'model.decoder.layers.7.final_layer_norm.bias',\n",
       " 'model.decoder.layers.8.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.8.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.8.fc1.bias',\n",
       " 'model.decoder.layers.8.fc2.bias',\n",
       " 'model.decoder.layers.8.final_layer_norm.weight',\n",
       " 'model.decoder.layers.8.final_layer_norm.bias',\n",
       " 'model.decoder.layers.9.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.9.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.9.fc1.bias',\n",
       " 'model.decoder.layers.9.fc2.bias',\n",
       " 'model.decoder.layers.9.final_layer_norm.weight',\n",
       " 'model.decoder.layers.9.final_layer_norm.bias',\n",
       " 'model.decoder.layers.10.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.10.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.10.fc1.bias',\n",
       " 'model.decoder.layers.10.fc2.bias',\n",
       " 'model.decoder.layers.10.final_layer_norm.weight',\n",
       " 'model.decoder.layers.10.final_layer_norm.bias',\n",
       " 'model.decoder.layers.11.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.11.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.11.fc1.bias',\n",
       " 'model.decoder.layers.11.fc2.bias',\n",
       " 'model.decoder.layers.11.final_layer_norm.weight',\n",
       " 'model.decoder.layers.11.final_layer_norm.bias',\n",
       " 'model.decoder.layers.12.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.12.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.12.fc1.bias',\n",
       " 'model.decoder.layers.12.fc2.bias',\n",
       " 'model.decoder.layers.12.final_layer_norm.weight',\n",
       " 'model.decoder.layers.12.final_layer_norm.bias',\n",
       " 'model.decoder.layers.13.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.13.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.13.fc1.bias',\n",
       " 'model.decoder.layers.13.fc2.bias',\n",
       " 'model.decoder.layers.13.final_layer_norm.weight',\n",
       " 'model.decoder.layers.13.final_layer_norm.bias',\n",
       " 'model.decoder.layers.14.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.14.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.14.fc1.bias',\n",
       " 'model.decoder.layers.14.fc2.bias',\n",
       " 'model.decoder.layers.14.final_layer_norm.weight',\n",
       " 'model.decoder.layers.14.final_layer_norm.bias',\n",
       " 'model.decoder.layers.15.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.15.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.15.fc1.bias',\n",
       " 'model.decoder.layers.15.fc2.bias',\n",
       " 'model.decoder.layers.15.final_layer_norm.weight',\n",
       " 'model.decoder.layers.15.final_layer_norm.bias',\n",
       " 'model.decoder.layers.16.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.16.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.16.fc1.bias',\n",
       " 'model.decoder.layers.16.fc2.bias',\n",
       " 'model.decoder.layers.16.final_layer_norm.weight',\n",
       " 'model.decoder.layers.16.final_layer_norm.bias',\n",
       " 'model.decoder.layers.17.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.17.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.17.fc1.bias',\n",
       " 'model.decoder.layers.17.fc2.bias',\n",
       " 'model.decoder.layers.17.final_layer_norm.weight',\n",
       " 'model.decoder.layers.17.final_layer_norm.bias',\n",
       " 'model.decoder.layers.18.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.18.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.18.fc1.bias',\n",
       " 'model.decoder.layers.18.fc2.bias',\n",
       " 'model.decoder.layers.18.final_layer_norm.weight',\n",
       " 'model.decoder.layers.18.final_layer_norm.bias',\n",
       " 'model.decoder.layers.19.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.19.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.19.fc1.bias',\n",
       " 'model.decoder.layers.19.fc2.bias',\n",
       " 'model.decoder.layers.19.final_layer_norm.weight',\n",
       " 'model.decoder.layers.19.final_layer_norm.bias',\n",
       " 'model.decoder.layers.20.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.20.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.20.fc1.bias',\n",
       " 'model.decoder.layers.20.fc2.bias',\n",
       " 'model.decoder.layers.20.final_layer_norm.weight',\n",
       " 'model.decoder.layers.20.final_layer_norm.bias',\n",
       " 'model.decoder.layers.21.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.21.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.21.fc1.bias',\n",
       " 'model.decoder.layers.21.fc2.bias',\n",
       " 'model.decoder.layers.21.final_layer_norm.weight',\n",
       " 'model.decoder.layers.21.final_layer_norm.bias',\n",
       " 'model.decoder.layers.22.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.22.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.22.fc1.bias',\n",
       " 'model.decoder.layers.22.fc2.bias',\n",
       " 'model.decoder.layers.22.final_layer_norm.weight',\n",
       " 'model.decoder.layers.22.final_layer_norm.bias',\n",
       " 'model.decoder.layers.23.self_attn.k_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.v_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.q_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn.out_proj.bias',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.weight',\n",
       " 'model.decoder.layers.23.self_attn_layer_norm.bias',\n",
       " 'model.decoder.layers.23.fc1.bias',\n",
       " 'model.decoder.layers.23.fc2.bias',\n",
       " 'model.decoder.layers.23.final_layer_norm.weight',\n",
       " 'model.decoder.layers.23.final_layer_norm.bias']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parms_of_dtype(prepared_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "025520ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cb1d55f",
    "outputId": "aaa59835-70f3-4c1b-8ee2-831769f2c25b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264.15104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model.get_memory_footprint()/1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00981764",
   "metadata": {
    "id": "596dfabc"
   },
   "source": [
    "### PEFT\n",
    "\n",
    "\"_ðŸ¤— PEFT (Parameter-Efficient Fine-Tuning) is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. PEFT methods only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware._\n",
    "\n",
    "_PEFT is integrated with the Transformers, Diffusers, and Accelerate libraries to provide a faster and easier way to load, train, and use large models for inference._\"\n",
    "\n",
    "****\n",
    "**Summary of \"PEFT\"**\n",
    "- the basic configuration below should work well in many cases\n",
    "```python\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, config)\n",
    "```\n",
    "- ranks of 8, 16, or 32 are typical, but using higher values shouldnâ€™t significantly impact the modelâ€™s memory footprint.\n",
    "- the scaling factor, `lora_alpha` is typically twice the rank.\n",
    "- if your model has `Conv1D` layers, add `fan_in_fan_out=True` to your configuration\n",
    "- if your model was recently released, you may need to specify the `target_modules` manually\n",
    "  - typically, use the names of the massive linear layers in the attention module.\n",
    "- by default, only the adapters are trainable\n",
    "  - if you'd like to train other layers, such as layer norms, add them to the `modules_to_save` argument\n",
    "  - if you're adding your own tokens to the tokenizer, you'll need to also train vocabulary-related layers such as embeddings and the model's head\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be9679cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d3a5286",
    "outputId": "d93cfb62-62df-4142-8604-5b90159e8dfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=None, inference_mode=False, r=8, target_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig()\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13211b1e",
   "metadata": {
    "id": "e0d9fa4e"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79b1ea",
   "metadata": {
    "id": "f32ceecf"
   },
   "source": [
    "### `target_modules`\n",
    "\n",
    "Since there are new models and architectures being released on a weekly basis, chances are that there is no preconfigured list of target layers in your currently installed version of the PEFT library. In this case, youâ€™ll be greeted with the following error:\n",
    "\n",
    "***\n",
    "`ValueError: Please specify `target_modules` in `peft_config``\n",
    "***\n",
    "\n",
    "Once you have the names, you can use yet another configuration argument: target_modules, which is either\n",
    "the name or a list of the names of the modules to which you want to apply the adapters.\n",
    "\n",
    "**Supported Models**\n",
    "\n",
    "If you'd like to check if a given model's architecture is already supported by the installed version of the `peft` package, you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3bd853",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c748efa3",
    "outputId": "0bde0859-967c-4128-b35f-5e9e1733484c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['t5', 'mt5', 'bart', 'gpt2', 'bloom', 'blip-2', 'opt', 'gptj', 'gpt_neox', 'gpt_neo', 'bert', 'roberta', 'xlm-roberta', 'electra', 'deberta-v2', 'deberta', 'layoutlm', 'llama', 'chatglm', 'gpt_bigcode', 'mpt', 'RefinedWebModel', 'RefinedWeb', 'falcon', 'btlm', 'codegen', 'mistral', 'mixtral', 'stablelm', 'phi', 'gemma', 'qwen2'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.utils.constants import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a729f4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5fab1a77",
    "outputId": "21f925c4-e4ec-4878-ac31-09ac34346747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj', 'fc1', 'fc2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['phi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a83df",
   "metadata": {
    "id": "b66c058f"
   },
   "source": [
    "#### The PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c51eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "803f0bc8",
    "outputId": "af1336f5-4214-45f6-c892-011a17ca361c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config, adapter_name='default')\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe2884e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Erp2o_oLR9hg",
    "outputId": "82b3faaa-c60c-4008-c471-108839e4ed4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['q_proj', 'v_proj']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc0992b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b044f611",
    "outputId": "7a0531ba-3123-4896-ad1b-5a42f4d36b51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Linear4bit(\n",
       "  (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict(\n",
       "    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  )\n",
       "  (lora_B): ModuleDict(\n",
       "    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "  )\n",
       "  (lora_embedding_A): ParameterDict()\n",
       "  (lora_embedding_B): ParameterDict()\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj\n",
    "lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77fa2a75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5920c003",
    "outputId": "41c5694b-ffae-4bb7-c743-cf4b52d1146e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 786,432 || all params: 331,982,848 || trainable%: 0.2369\n"
     ]
    }
   ],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca0a102e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94b7c819",
    "outputId": "e5b9553d-f260-4cf4-84b3-1b47364f228a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.10.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.11.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.12.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.13.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.14.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.15.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.16.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.17.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.18.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.19.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.20.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.21.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.22.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.v_proj.lora_B.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_A.default.weight',\n",
       "  torch.float32),\n",
       " ('model.decoder.layers.23.self_attn.q_proj.lora_B.default.weight',\n",
       "  torch.float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_parms(peft_model.base_model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8fefa",
   "metadata": {
    "id": "6bacfhFjBS3W"
   },
   "source": [
    "#### `modules_to_save`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eb95263",
   "metadata": {
    "id": "03cDipGaBcH9"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57fb1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the model is modified in-place, we need to reload it \n",
    "# before applying a new LoRA configuration\n",
    "# model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "#                                                 device_map='auto',\n",
    "#                                                 torch_dtype=compute_dtype,\n",
    "#                                                 quantization_config=nf4_config)\n",
    "# prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "#                                         use_gradient_checkpointing=True,\n",
    "#                                         gradient_checkpointing_kwargs={'use_reentrant': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9052a377",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0H-u7yQB4bX",
    "outputId": "b279c992-a367-418b-d54e-db5cc7177e29"
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.93 GiB total capacity; 4.95 GiB already allocated; 12.50 MiB free; 5.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m peft_model\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/mapping.py:183\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    182\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/peft_model.py:1542\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/peft_model.py:156\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(model, {adapter_name: peft_config}, adapter_name)\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_additional_trainable_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39m_cast_adapter_dtype(\n\u001b[1;32m    160\u001b[0m         adapter_name\u001b[38;5;241m=\u001b[39madapter_name, autocast_adapter_dtype\u001b[38;5;241m=\u001b[39mautocast_adapter_dtype\n\u001b[1;32m    161\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/peft_model.py:886\u001b[0m, in \u001b[0;36mPeftModel.set_additional_trainable_modules\u001b[0;34m(self, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save\u001b[38;5;241m.\u001b[39mupdate(peft_config\u001b[38;5;241m.\u001b[39mmodules_to_save)\n\u001b[0;32m--> 886\u001b[0m \u001b[43m_set_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/utils/other.py:330\u001b[0m, in \u001b[0;36m_set_trainable\u001b[0;34m(model, adapter_name)\u001b[0m\n\u001b[1;32m    328\u001b[0m parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, ModulesToSaveWrapper):\n\u001b[0;32m--> 330\u001b[0m     \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     target\u001b[38;5;241m.\u001b[39mset_adapter(target\u001b[38;5;241m.\u001b[39mactive_adapter)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/peft/utils/other.py:235\u001b[0m, in \u001b[0;36mModulesToSaveWrapper.update\u001b[0;34m(self, adapter_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save\u001b[38;5;241m.\u001b[39mupdate(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleDict({adapter_name: \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moriginal_module\u001b[49m\u001b[43m)\u001b[49m}))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save[adapter_name], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     old_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules_to_save[adapter_name]\u001b[38;5;241m.\u001b[39m_hf_hook\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:270\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 270\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    272\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:270\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 270\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    272\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pyt20/lib/python3.9/site-packages/torch/nn/parameter.py:55\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m     56\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.93 GiB total capacity; 4.95 GiB already allocated; 12.50 MiB free; 5.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# only \n",
    "_ = peft_model.unload()\n",
    "\n",
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a620eaf7",
   "metadata": {
    "id": "gWjw689CU3Jr"
   },
   "source": [
    "#### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13f7e0b2",
   "metadata": {
    "id": "0N9eRMkrlmjk"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=['layer_norm', 'embed_tokens']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = peft_model.unload()\n",
    "# since the model is modified in-place, we need to reload it \n",
    "# before applying a new LoRA configuration\n",
    "# model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "#                                                 device_map='auto',\n",
    "#                                                 torch_dtype=compute_dtype,\n",
    "#                                                 quantization_config=nf4_config)\n",
    "# prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "#                                         use_gradient_checkpointing=True,\n",
    "#                                         gradient_checkpointing_kwargs={'use_reentrant': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80178211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QIsT1wU4oHRm",
    "outputId": "f6fd4f76-8e21-42f0-edc2-b8261e292ab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 26,624,000 || all params: 357,820,416 || trainable%: 7.4406\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7735c",
   "metadata": {
    "id": "geZhlw7sUwcj"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['embed_tokens', 'q_proj', 'v_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d01087",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = peft_model.unload()\n",
    "# since the model is modified in-place, we need to reload it \n",
    "# before applying a new LoRA configuration\n",
    "# model_q4 = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\",\n",
    "#                                                 device_map='auto',\n",
    "#                                                 torch_dtype=compute_dtype,\n",
    "#                                                 quantization_config=nf4_config)\n",
    "# prepared_model = prepare_model_for_kbit_training(model_q4,\n",
    "#                                         use_gradient_checkpointing=True,\n",
    "#                                         gradient_checkpointing_kwargs={'use_reentrant': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a09a856",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGJCk10poZVK",
    "outputId": "ebd09991-c551-4147-9ff1-f2f7ac5f340b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,598,976 || all params: 332,893,696 || trainable%: 0.4803\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(prepared_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f13bade5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_NaR4Dht8kV",
    "outputId": "af5bc86e-11a2-466a-cf22-ca13771dcc0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lora.Embedding(\n",
       "  (base_layer): Embedding(50272, 512, padding_idx=1)\n",
       "  (lora_dropout): ModuleDict(\n",
       "    (default): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (lora_A): ModuleDict()\n",
       "  (lora_B): ModuleDict()\n",
       "  (lora_embedding_A): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 8x50272 (cuda:0)])\n",
       "  (lora_embedding_B): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 512x8 (cuda:0)])\n",
       "  (lora_magnitude_vector): ModuleDict()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin = peft_model.base_model.model.model.decoder.embed_tokens\n",
    "lin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875a42d",
   "metadata": {
    "id": "6117cc67"
   },
   "source": [
    "#### Managing Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "46154ae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUNt_ztabRqT",
    "outputId": "7e74c29f-7089-490e-f2cd-99904997c0b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.load_adapter('dvgodoy/opt-350m-lora-yoda', adapter_name='yoda')\n",
    "lora_A = peft_model.base_model.model.model.decoder.layers[0].self_attn.q_proj.lora_A\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9cfaf80a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0YwcI9jb1gv",
    "outputId": "7b6ec260-4988-465c-ea76-759acc5ab81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (third): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.add_adapter(adapter_name='third', peft_config=config)\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2a9d787c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1xd9TlweD8Y",
    "outputId": "3bd55add-b059-4be1-85b5-5516f2089e4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.delete_adapter(adapter_name='third')\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60106701",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FruiHumqfD44",
    "outputId": "580075aa-669d-41e9-fa3f-c779cfc4caa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['default', 'yoda'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e69e9f7c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "s94-okx0gtPH",
    "outputId": "ea949b66-f88f-41b0-83de-2dc3d83aa7b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "563a806c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3RUEmEmog7M6",
    "outputId": "a166dbb0-ccfe-49b5-dde2-d349290566ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'yoda'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.set_adapter('yoda')\n",
    "peft_model.active_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5f03c6",
   "metadata": {
    "id": "0sLQjTnWcNA3"
   },
   "source": [
    "```python\n",
    "with peft_model.disable_adapter():\n",
    "    original_outputs = peft_model(inputs)\n",
    "\n",
    "original_outputs = peft_model.base_model(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8358ebf9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBy_bjeOiTtE",
    "outputId": "00aa69eb-f21e-4f5e-e9ec-8c054ebe06b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "  (yoda): Linear(in_features=1024, out_features=8, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.merge_adapter(adapter_names=['yoda'])\n",
    "lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cdea3313",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5-0yrAbkOgW",
    "outputId": "ce884503-0035-4640-b9be-f9b3a84cc0c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): OPTForCausalLM(\n",
       "      (model): OPTModel(\n",
       "        (decoder): OPTDecoder(\n",
       "          (embed_tokens): Embedding(50272, 512, padding_idx=1)\n",
       "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 1024)\n",
       "          (project_out): Linear4bit(in_features=1024, out_features=512, bias=False)\n",
       "          (project_in): Linear4bit(in_features=512, out_features=1024, bias=False)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x OPTDecoderLayer(\n",
       "              (self_attn): OPTAttention(\n",
       "                (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=50272, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.unload()\n",
    "peft_model.base_model.model.model.decoder.layers[0].self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda37040",
   "metadata": {},
   "source": [
    "### Coming Up in \"Fine-Tuning LLMs\"\n",
    "\n",
    "Low-rank adapters saved the day by swooping in and enabling fast and cheap fine-tuning for LLMs. These humongous models, although powerful, are masters of a single tradeâ€”predicting the next tokenâ€”thus remaining limited by the structure of their inputs. A new kind of input must be developed to enable these creatures to chat. Learn more about the incredible tale of chat templates in the next chapter of \"Fine-Tuning LLMs.\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
